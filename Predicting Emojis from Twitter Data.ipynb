{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Emojis from Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emoji have become more and more prominent in today’s social media. Since their initial appearance in Japan in the 1990s, it has been found that emoji are used by over ninety-two percent of the online population in 2015 [1]. Due to the indicated trend, numerous NLP applications can benefit from the emoji interpretation capability.\n",
    "\n",
    "In this project, we aim to implement and to train the following models: a bidirectional LSTM, a CNN, and a bag of words. Our objective is to predict one of 5, 10, and 20 most frequently used emoticons for a given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We acquired a pre-processed dataset containing 584,600 tweets, posted between October 2015 and May 2016 in the US [2]. The dataset consists of three sets containing tweets from the top 5, 10, and 20 most common emojis. Each set is split into training, validation, and test sets with the training sets containing 2-5 hundred thousand tweets and the validation and test sets containing a couple ten thousand.\n",
    "\n",
    "Preprocessing consisted of replacing user mentions with the symbol \"@user\", as well as replacing words that occur less than 5 times with the symbol \"< unk >\". Punctuation such as commas and quotation marks are separated from words with a space and are treated as words themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline classifier, we have a bag of words classifier in which each message is represented as a vector of the most informative tokens selected using term frequency--inverse document frequency (TF-IDF). L2 regularized logistic regression is used to make the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another model noted to do well is a convolutional neural network[2][3]. The model consisted of passing 64 filters of width 3, 4, and 5 over a sequence of word embeddings (of dimension 50) which a max pool is applied to produce a fixed size output. The output then fed directly into a fully connected softmax used to predict the emoji class. During training the fully connected layer is subjected to dropout. Embeddings were initialized using pre-trained GloVe embeddings from twitter data. Words without matching GloVe embeddings were initialized from a uniform distribution from -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/cnn_model.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the basic CNN, increased fully connected layers and a highway network was introduced between the convolutional layer output and fully connected layer. Deep highway networks are noted to have improved training time over deep neural networks as well as produce similar outputs between semantically similar words and phrases with vastly different input[3]. A highway layer is defined by eq. 1 where $\\circ$ is an element wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\text{relu}(W_H x + b_H) \\circ \\sigma(W_T x + b_T) + (1 - \\sigma(W_T x + b_T)) \\circ x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of a highway network is of the same dimension as its inputs, $W_H$ and $W_T$ are therefore square matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is applied between every layer from the convolutional output up to, but not including, the softmax layer in order to regularize the model. Weights are initialized using the Glorot uniform distribution. Biases are initialized to 0 except for $b_T$ which is initialized from a uniform distribution from -4 to -2. This is so highway networks tend to produce similar output as its inputs at first. All models were trained using the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Long-Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore a bi- and uni-directional LSTM model to our sequence classification problem with GloVe word embeddings.\n",
    "\n",
    "LSTM neural networks are being actively researched as they show promising results and can provide state-of-the-art performance. Recently engineers at Google greatly improved their voice recognition and transcription systems by incorporating LSTM RNNs that outperform DNNs and RNNs. (https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf).\n",
    "\n",
    "At the onset of the development, I started my implementation in Pytorch framework. I found tutorials that provided in-depth information that helped me understand embeddings and neural nets in NLP more in more detail. Additionally, I analyzed a word language model from Pytorch tutorials. However, there was a big surprise to running this model. Training the system took lots of time: about 4 hours per epoch or 17 min for a mini batch. My solution was to acquire access CUDA-enabled GPU to perform all computations in parallel. Doing so allowed to speed up the process by roughly 56 times.\n",
    "\n",
    "However, I decided to switch to a more high-level framework, Keras, to envision the structure in a simple way and to be consistent on the software with my team.\n",
    "I followed the approach in the paper Are Emojis Predictable by  Francesco Barbieri. Since many details of the implementation were omitted, I conducted many experiments to find this particular solution. Firstly, I tokenize, enumerate, and pad or truncate each tweet to the uniform length of 35. Than I use GloVe word embeddings to represent all the words in the training set as the first hidden layer of the network. It takes input dimension (vocabulary size), output dimension (size of the embedded vector for each word = 100), and input length( max length of each tweet = 35 ). The embedding weights are also learned. I introduce dropout layer where random neuron get dropped out during training with a given probability to prevent overfitting. Initially I used just LSTM layer, but adding Bidirectional LSTM improved the results for the set of 5 emoji. This is because BILSTM can provide more context and improve learning as it uses input in both forward and backward directions. This layer has 64 hidden units as this number seem to work best. The final stage output enters the softmax function to determine the most probably emoji. Additionally, I use a popular in NLP optimization algorithm called Adam. This method computes different learning rates for various parameters unlike traditional SGD where learning rate remains unchanged.\n",
    "\n",
    "Testing showed that my BILSTM/LSTM  model does not perform as well as the baseline model. However, my model outperform the one described in the paper. Similarly to CNN model, I noticed that BILSTM leans towards favoring the most frequently used emoji, specifically “tears of joy.” This is due to uneven number of training samples for each emoji. I could further improve accuracy by splitting or eliminating hashtag concatenated words. Overall, I consider my work a success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested our models using a weighted F1 score as an indicator of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>F1 Scores by Model per Top N Emojis</b></center>\n",
    "\n",
    "|  | baseline | CNN | Resampled CNN | Highway CNN | LSTM | Bi-LSTM |\n",
    "|--|----------|----------|----------|----------|----------|----------|\n",
    "|5 | 0.592061 | 0.549705 | <b>0.595257</b> | 0.564256 | 0.59 ||\n",
    "|10| 0.441736 | <b>0.447219</b> | 0.390082 | 0.423835 | 0.44 ||\n",
    "|20| <b>0.347743</b> | 0.208166 | 0.292820 | 0.284491 | 0.34 ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, as one might notice none of our models performed siginificantly better than our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN | Resampled CNN\n",
    ":-:|:-:\n",
    "<img src=images/5_confusion.png width=250p> | <img src=images/5_resample.png width=250p>\n",
    "\n",
    "<b>Figure 2.</b> Confusion matrix of top 5 emojis for various CNNs. The most common emoji is denoted as the class 0 while the least common is denoted as the class 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can compare the results of the various CNNs. As one would expect, the resampled CNN does a better job at predicting the less common classes, however this is to the detriment of now confusing the first and third most common emojis together. However both of these perform better than a CNN with a single highway layer since it fails to distinguish the thir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: http://emogi.com/documents/Emoji_Report_2015.pdf \"Emoji Report 2015\", emoji.com, 2015\n",
    "\n",
    "[2]: https://arxiv.org/pdf/1702.07285.pdf F. Barbieri, M. Ballesteros, H. Saggion, \"Are Emojis Predictable?\", 2016\n",
    "\n",
    "[3]: https://web.stanford.edu/class/cs224n/reports/2762064.pdf L. Zhao, C. Zeng, \"Using Neural Networks to Predict Emoji Usage from Twitter Data\"\n",
    "\n",
    "[4]: https://arxiv.org/abs/1408.5882 Y. Kim, \"Convolutional Neural Networks for Sentence Classification\", 2014\n",
    "\n",
    "[5]: https://arxiv.org/abs/1505.00387 R. Srivastava, K. Greff, J. Schmidhuber, \"Highway Networks\", 2015 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/neonrights/emoji_predictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
